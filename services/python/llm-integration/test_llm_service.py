#!/usr/bin/env python3
"""
Test script for LLM Integration Service
"""
import asyncio
import sys
import os
import json

# Add shared modules to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))

from config import LLMIntegrationConfig
from llm_providers import LLMProviderManager
from test_generator import IntelligentTestGenerator
from prompt_manager import PromptManager
from rate_limiter import RateLimiter
from models import LLMRequest, LLMProvider


async def test_llm_service():
    """Test LLM service functionality"""
    print("ğŸ§ª Testing LLM Integration Service...")
    
    # Initialize configuration
    config = LLMIntegrationConfig()
    print(f"ğŸ“‹ Service: {config.service_name}")
    print(f"ğŸ”‘ Azure Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT', 'Not set')}")
    
    # Test LLM Provider Manager
    print("\n1ï¸âƒ£ Testing LLM Provider Manager...")
    llm_manager = LLMProviderManager(config)
    
    try:
        await llm_manager.test_connections()
        print("âœ… LLM providers initialized successfully!")
    except Exception as e:
        print(f"âŒ LLM provider initialization failed: {e}")
        return False
    
    # Test provider status
    print("\n2ï¸âƒ£ Testing provider status...")
    try:
        status = await llm_manager.get_provider_status()
        print(f"âœ… Provider status retrieved: {list(status.keys())}")
        for provider, provider_status in status.items():
            print(f"   ğŸ“Š {provider}: {provider_status}")
    except Exception as e:
        print(f"âŒ Provider status check failed: {e}")
        return False
    
    # Test available providers
    print("\n3ï¸âƒ£ Testing available providers...")
    try:
        providers = await llm_manager.get_available_providers()
        print(f"âœ… Available providers: {len(providers)}")
        for provider in providers:
            print(f"   ğŸ¤– {provider['display_name']}: {provider['available']}")
            print(f"      Models: {', '.join(provider['models'][:3])}...")
    except Exception as e:
        print(f"âŒ Available providers check failed: {e}")
        return False
    
    # Test basic text generation
    print("\n4ï¸âƒ£ Testing basic text generation...")
    try:
        request = LLMRequest(
            provider=LLMProvider.OPENAI,
            model="gpt-4",
            prompt="Generate a simple test case for a login button. Keep it brief.",
            max_tokens=200,
            temperature=0.3
        )
        
        response = await llm_manager.generate_text(request)
        print("âœ… Text generation successful!")
        print(f"   ğŸ“ Generated text: {response.content[:100]}...")
        print(f"   ğŸ¯ Tokens used: {response.tokens_used}")
        print(f"   â±ï¸  Processing time: {response.processing_time:.2f}s")
        
    except Exception as e:
        print(f"âŒ Text generation failed: {e}")
        return False
    
    # Test Prompt Manager
    print("\n5ï¸âƒ£ Testing Prompt Manager...")
    prompt_manager = PromptManager()
    
    try:
        prompts = await prompt_manager.get_available_prompts()
        print(f"âœ… Prompt templates loaded: {len(prompts)}")
        
        for prompt in prompts[:3]:  # Show first 3
            print(f"   ğŸ“‹ {prompt['name']}: Variables: {prompt['variables']}")
        
    except Exception as e:
        print(f"âŒ Prompt manager test failed: {e}")
        return False
    
    # Test edge case generation
    print("\n6ï¸âƒ£ Testing edge case generation...")
    test_generator = IntelligentTestGenerator(config, llm_manager)
    
    try:
        edge_cases = await test_generator.generate_edge_cases(
            feature_description="User login form with email and password fields",
            existing_tests=[
                {"name": "Valid login", "type": "positive"},
                {"name": "Invalid password", "type": "negative"}
            ],
            provider="openai",
            model="gpt-4"
        )
        
        print(f"âœ… Edge cases generated: {len(edge_cases)}")
        if edge_cases:
            first_case = edge_cases[0]
            print(f"   ğŸ§ª Sample edge case: {first_case.name}")
            print(f"   ğŸ“‹ Description: {first_case.description[:80]}...")
            print(f"   ğŸ“ Steps: {len(first_case.steps)} steps")
        
    except Exception as e:
        print(f"âŒ Edge case generation failed: {e}")
        return False
    
    # Test rate limiting
    print("\n7ï¸âƒ£ Testing rate limiting...")
    rate_limiter = RateLimiter(config)
    
    try:
        # Test rate limit check
        allowed = await rate_limiter.check_rate_limit("generate", "openai")
        print(f"âœ… Rate limit check: {'Allowed' if allowed else 'Blocked'}")
        
        # Get rate limit status
        status = await rate_limiter.get_rate_limit_status("openai")
        minute_status = status["current_minute"]
        print(f"   ğŸ“Š Current minute: {minute_status['count']}/{minute_status['limit']} requests")
        
    except Exception as e:
        print(f"âŒ Rate limiting test failed: {e}")
        return False
    
    # Test custom prompt execution
    print("\n8ï¸âƒ£ Testing custom prompt execution...")
    try:
        result = await prompt_manager.execute_prompt(
            template_name="edge_case_generation",
            variables={
                "feature_description": "Search functionality",
                "existing_tests": json.dumps([{"name": "Basic search", "type": "functional"}]),
                "test_count": "2"
            },
            provider="openai",
            model="gpt-4",
            llm_manager=llm_manager
        )
        
        print("âœ… Custom prompt execution successful!")
        print(f"   ğŸ“ Response length: {len(result['response'])} characters")
        print(f"   ğŸ¯ Tokens used: {result['tokens_used']}")
        
    except Exception as e:
        print(f"âŒ Custom prompt execution failed: {e}")
        return False
    
    print("\nğŸ‰ All LLM service tests passed!")
    return True


async def test_integration_scenario():
    """Test a complete integration scenario"""
    print("\n" + "="*60)
    print("ğŸ”— TESTING COMPLETE INTEGRATION SCENARIO")
    print("="*60)
    
    config = LLMIntegrationConfig()
    llm_manager = LLMProviderManager(config)
    test_generator = IntelligentTestGenerator(config, llm_manager)
    
    try:
        # Simulate generating tests from requirements
        print("\nğŸ“‹ Testing requirements-to-tests generation...")
        
        sample_requirements = """
        User Story: As a customer, I want to search for products so that I can find what I need quickly.
        
        Acceptance Criteria:
        - Search box is prominently displayed on homepage
        - User can enter search terms and press Enter or click search button
        - Results are displayed within 2 seconds
        - No results found message appears if no matches
        - Search terms are highlighted in results
        """
        
        test_suite = await test_generator.generate_from_requirements(
            requirements_text=sample_requirements,
            target_url="https://example-shop.com",
            provider="openai",
            model="gpt-4"
        )
        
        print("âœ… Requirements-to-tests generation successful!")
        print(f"   ğŸ“Š Test scenarios generated: {len(test_suite.scenarios)}")
        print(f"   ğŸ§ª UI tests generated: {len(test_suite.ui_tests)}")
        print(f"   ğŸ“ Test suite: {test_suite.name}")
        
        if test_suite.scenarios:
            first_scenario = test_suite.scenarios[0]
            print(f"   ğŸ¯ Sample scenario: {first_scenario.name}")
            print(f"   ğŸ“‹ Steps: {len(first_scenario.steps)} steps")
        
        return True
        
    except Exception as e:
        print(f"âŒ Integration scenario failed: {e}")
        return False


async def main():
    """Main test function"""
    print("=" * 60)
    print("ğŸš€ LLM INTEGRATION SERVICE TEST")
    print("=" * 60)
    
    try:
        # Basic service tests
        success = await test_llm_service()
        if not success:
            print("\nâŒ Basic LLM service tests failed!")
            exit(1)
        
        # Integration scenario test
        integration_success = await test_integration_scenario()
        if not integration_success:
            print("\nâŒ Integration scenario tests failed!")
            exit(1)
        
        print("\nâœ… All LLM Integration Service tests passed!")
        print("ğŸ¯ Service is ready for production use!")
        exit(0)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Test interrupted by user")
        exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())